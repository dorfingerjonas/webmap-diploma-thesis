\section{Versionierung}
\setauthor{Jonas Dorfinger}
Der Begriff Versionierung beschreibt im Allgemeinen einen Prozess welcher zur Dokumentation von Änderungen an Dokumenten oder Dateien stattfindet.
Nach jeder Änderung wird im System eine aktuelle Version abgespeichert und mit einer eindeutigen Versionsnummer versehen.
Die Versionsnummer wird meist automatisch durch das Versionierungstool vergeben.
Zusätzlich wird auch gespeichert, wer welche Einträge wann und wo gemacht hat, so werden Änderungen transparent mitprotokolliert.
Eine der wichtigsten Vorteile einer Versionierung ist jedoch die Funktion, dass man alte Ständ von Dateien wiederherstellen kann.
Eine Versionsverwaltungssoftware kann auch die gleichzeitigen Zugriffe auf eine Datei von mehrere Personen koordinieren.
Versionierung ist nicht nur in der Softwareentwicklungs Branche Standard, sondern auch in etlichen weiteren Bereichen stark
verbreitet.

\subsection{automatische vs. manuelle Versionierung}
\setauthor{Jonas Dorfinger}
Wenn während dem Schreiben einer Arbeit Sicherheitskopien anlegt und diese mit zum Beispiel "arbeit-neu",
"arbeit-fertig" oder "arbeit-fertig2" benennt werden, dann ist das nichts anderes als eine manuelle Versionierung, weil auch hier
eine eindeutige Versionsnummer für jede Datei vergeben wurde.
Das birgt aber auch große Risiken, da eine manuelle Versionierung viel Zeit und Disziplin erfordert und immer die Gefahr
besteht, dass man eine wichtige Version überschreibt.
Die meisten Probleme werden von der automatischen Versionierung gelöst.
Bei dieser Variante werden automatisch, wenn man einen Stand als "fertig" markiert, diese mit einer Versionsnummer versehen
und im Archiv schreibgeschützt abgelegt.
Dadurch hat man die Risiken der manuellen Versionierung auf ein Minimum reduziert~\cite{versionierung}.

\subsection{git}
\setauthor{Jonas Dorfinger}
git ist \emph{das} Versionierungssystem in der Softwareentwicklung.
Es wurde designt um kleine aber auch extrem große und komplexe Projekte effizient zu verwalten.
Grundsätzlich als CLI (Command Line Interface) konzeptioniert, gibt es mittlerweile auch etliche Grafische Implementierungen,
um git interaktiver und intuitiver zu gestalten.
Laut einer Umfrage von Stackoverflow nutzen mehr als 87\% der Entwickler weltweit git~\cite{stackoverflow-git-survey}.

\subsubsection{Der git Workflow}
Der git Workflow beschreibt die effizienteste Arbeitsweise mit git.
Dabei gibt es vier Stufen zwischen denen mit verschiedenen Konsolen Befehlen gewechselt werden kann.

\paragraph{1. Remote Repository}
Das Remote Repository befindet sich auf einem externen Server (zum Beispiel Github Server~\ref{subsec:github}).
Mit einem einfache \emph{git clone} Befehl in der Konsole, kann man das Repository klonen und somit ein lokales Repository erstellen.
Auf dieses Archiv können mehrere Personen zugreifen und so gemeinsam an einem Projekt arbeiten.

\paragraph{2. Lokales Repository}
Das Lokale Repository ist eine Kopie der Version im Remote Repository, mit dem Unterschied, dass die lokalen Änderungen in
diesem Repository gesammelt werden und erst durch den Befehl \emph{git push} wieder zurück ins Remote Repository gelangen.

\paragraph{3. Staging Area}
In der Staging Area sind alle lokal modifizierten Files, welche für den nächsten Commit auserwählt sind, gesammelt.
Durch den Befehl \emph{git commit -m <message>} gelangen ausgewählte Änderungen in des lokale Repository.
Ein Commit repräsentiert dabei eine Version des Programmes, welche mit einer eindeutigen Nummer im System gespeichert wird.

\paragraph{4. Working Directory}
Im Working Directory wird tatsächlich programmiert, alle Änderungen der Files werden hier durchgeführt.
Mit dem Befehl \emph{git add <filename>} können einzelne Files gezielt in die Staging Area verschoben werden um diese
dann anschließend ins lokale Repository zu committen.

\begin{figure}[hbt!]
    \centering
    \includegraphics[scale=0.4]{pics/git-workflow}
    \caption{git workflow~\cite{git-workflow}}
    \label{fig:git-workflow}
\end{figure}

\cite{git-workflow}

\subsection{Semantische Versionierung}
\label{subsec:semantic-versioning}
Eine semantische Versionsnummer besteht aus drei Teilen:

\begin{center}
    \centering{MAJOR.MINOR.PATCH}
\end{center}

Und sollen nach Änderungen in der entsprechenden Kategorie in einser Schritten erhöht werden.

\textbf{MAJOR}
Wenn API Änderungen durchgeführt werden, welche die API mit der Vorgängerversion inkompatibel machen
\linebreak

\textbf{MINOR}
Wenn Funktionalität hinzugefügt wird, welche mit älteren Versionen noch kompatibel sind
\linebreak

\textbf{PATCH}
Wenn kleine Bug fixes durchgeführt werden, welche auch mit älteren Versionen kompatibel sind
\linebreak

Zusätzlich können auch labels für \emph{pre-release} oder \emph{build metadata} angegeben werden:

\begin{center}
    \centering{MAJOR.MINOR.PATCH format}
\end{center}

\cite{semantische-versionierung}

\subsection{GitHub}\label{subsec:github}
GitHub Inc. ist ein amerikanisches gewinnorientiertes Softwareunternehmen, welches git Repositories in der Cloud anbietet,
also das Remote Repository.
Dadurch können Privatpersonen aber auch große Unternehmen und Konzerne ganz leicht git als Versionierungstool verwenden.
Durch viele Features von GitHub wird das arbeiten mit git so stark erleichtert, dass auch Anfänger bereits sehr profesionell
mit git arbeiten und lernen können.
Normalerweise verwendet man git mit der Kommandozeile des jeweiligen Betriebssystemes, was hohes technisches Verständnis voraussetzt.
Man kann einen kostenlosen Account erstellen und gratis Repositories anlegen und verwenden,
deshalb ist Github in der Open-Source Community sehr stark verbreitet.
Doch mittlerweile kann diese Software mehr als "nur" Projekte zu versionieren.
Es gibt project-tracking-tools (GitHub Projects, Issues, Milestones, Labels, etc.), Pipelines für Continues Integration and Delivery,
gratis Website Hosting und noch vieles mehr~\cite{github-features}.

Es gibt aber auch andere Programme welche zu GitHub gehören, zum einen ist es \emph{Atom}, ein kostenloser und Open-Source Editor für Entwickler.
Weiters gibt es zum Beispiel auch \emph{Electron}, das ist ebenfalls ein Open-Source JavaScript Framework um Web-Anwendungen zu Desktopanwendungen zu machen.

Unternehmen können kostenpflichtige Organisationen anlegen, um alle Repositories einer Firma gesammelt an einem Ort zu verwalten.
Dazu gibt es wieder etliche Features welche die Sicherheit, Rollen und vieles mehr betreffen.

Seit 2018 gehört GitHub zu Microsoft, welche für die Übernahme \$7,5 Milliarden US-Dollar auf den Tisch gelegt haben~\cite{microsoft-acquires-github}.


\section{Projektkoordination}
\setauthor{Jonas Dorfinger}
Da Triply über eine GitHub Organisation verfügt, ist es naheliegend, dass für webmap ein eigenes Repository angelegt wird.

Um Übersicht über den Projektverlauf zu behalten, ist die Entscheidung aufgrund von internen Vereinbarungen von triply sowie die
Erfahrung des Teams auf GitHub Projects gefallen.
Man kann ein Project Board ganz einfach über die GitHub Website erstellen und dabei auswählen, ob eine Vorlage ausgewählt werden soll.

Es gibt folgende Templates zur Auswahl: ~\cite{github-create-project}

\paragraph{None}
Es wird ein völlig leeres Project Board erstellt, alle Spalten und Einstellungen müssen manuell gemacht werden.

\paragraph{Basic Kanban}
Ein Project Board mit den Spalten \emph{To do}, \emph{In progress} und \emph{Done} wird erstellt.

\paragraph{Automated Kanban}
Es wird ein Basic Kanban Board erstellt, mit der zusätzlichen Funktion von eingebauten Triggern, welche automatisch Issues
und Pull Requests zwischen den Spalten hin und her schieben.

\paragraph{Automated Kanban with Reviews}
Das Template \emph{Automated Kanban with Reviews} erweitert das Automated Kanban Template um 2 weitere Spalten und einen Trigger
, für die Pull Request Reviews.

\paragraph{Bug triage}
Dieses Template wird verwendet, um Bugs zu priorisieren und diese geordnet anzusehen, dabei gibt es folgende Spalten:

\begin{itemize}
    \item To do
    \item High priority
    \item Low priority
    \item Closed
\end{itemize}

\subsection{Issues}
Issues haben die Möglichkeit, die anstehende Arbeit zu dokumentieren und planen.

\cite{github-issues}

\subsection{Automatisierung}
Durch die Verwendung spezieller Keywords in Commit messages oder in einem Kommentar bei einem Issue, kann der Zustand dieses
Issues verändert werden.
Dies geht einher mit der Automatisierung von Project Boards.

\textbf{Keywords:}

\begin{itemize}
    \item close
    \item closes
    \item closed
    \item fix
    \item fixes
    \item fixed
    \item resolve
    \item resolves
    \item resolved
\end{itemize}

\textbf{Verwendung}

\begin{center}
    \centering{<Keyword> <Issue Nummer> <Commit Message>}
\end{center}

\cite{github-issue-automation}


\section{Frontend Framework/Library}
\label{sec:frontend-framework/library}
\input{sections/frontend-framework-library}


\section{Map Frameworks}
\setauthor{Jonas Dorfinger}
Geo-Daten sind sehr komplexe Daten, welche nur sehr schwer und mit viel Erfahrung ohne Visualisierung interpretiert werden können.
Um solche Daten visualiseren zu können werden Map Frameworks benötigt, diese können auf digitalen Karten, die Koordinaten
grafisch darstellen, doch auch interaktive Darstellungen sind möglich.
Drei der bekanntesten Frameworks sind:

\begin{itemize}
    \item Mapbox
    \item Leaflet
    \item Google Maps
\end{itemize}

\subsection{Mapbox}
Mapbox ist ein amerikanisches Unternehmen, welches sich auf Custom-Maps spezialisiert und die Nische stark vergrößert hat.
Dabei geht es um die Darstellung von Geo-Daten auf digitalen Kartensystemen und deren Individualisierung.
Weiters ist Mapbox Contributor zum OpenStreetMap (OSM) Projekt, das ist eine Geo-Datenbank mit Kartendaten und Informationen
für den gesamten Globus.
Große Konzerne wie Amazon, Facebook oder Snapchat setzen auf OpenStreetMap für ihre eigenen Angebote und Services den Kunden gegenüber.
Unternehmen wie Tesla, mit einer eigenen Navigationssoftware verwenden die OSM-Datenbank ebenfalls für ihre eigenen Zwecke~\cite{osm-customers-1, osm-customers-2}.

\begin{figure}[hbt!]
    \centering
    \includegraphics[scale=0.25]{pics/austria-mapbox}
    \caption{Österreich mit Bundesländern in Mapbox}
    \label{fig:austria-mapbox}
\end{figure}

\paragraph{Vorteile}
\begin{itemize}
    \item OpenSource (bis Version 2.0.0)~\cite{mapbox-open-source}
    \item Gutes Handling von großen Datensätzen
    \item Individualisierbar
    \item Gute Unterstützung von komplexen Strukturen
\end{itemize}

\paragraph{Nachteile}
\begin{itemize}
    \item Ausbaufähige Dokumentation
    \item Kein ausgereifter Angular Wrapper
\end{itemize}

\subsection{Leaflet}
Leaflet ist das laut eigenen Angaben die führende Open-Source JavaScript Library, wenn mobile und interaktive Karten gefordert sind.
Durch die sehr gute API Dokumentation können ganz leicht Geo-Daten auf Karten dargestellt und interaktive Elemente eingebaut werden.
Der Fokus liegt dabei auf den Grundfunktionalitäten, das beinhaltet auch eine Performance Optimierung für fast alle Geräte und Plattformen.
Dank einer leichten Anbindungsmöglichkeit für Plugins, gibt es etliche davon im Internet, die man einfach verwenden kann,
um die Funktionalitäten zu erweitern.
Leaflet wird auch von den ganz großen verwendet: GitHub, Pinterest, Facebook, European Commission, The Washington Post und noch viele mehr.
~\cite{leaflet-doc}

\begin{figure}[hbt!]
    \centering
    \includegraphics[scale=0.4]{pics/austria-leaflet}
    \caption{Österreich mit Bundesländern in Leaflet}
    \label{fig:austria-leaflet}
\end{figure}

\paragraph{Vorteile}
\begin{itemize}
    \item Open-Source (keine Restriktion oder Terms of Service)
    \item Hervorragende Dokumentation
    \item Leichtgewichtig (39KB)
    \item Plugin Support
    \item Angular Wrapper
    \item Bereits von triply in Verwendung
\end{itemize}

\paragraph{Nachteile}
\begin{itemize}
    \item Offizielle Dokumentation beinhaltet nur grundlegende Beispiele
    \item Möglicherweise verwenden von GIS Programmen (zum Beispiel QGIS) notwendig, um Informationen aufzubereiten
\end{itemize}

\subsection{Google Maps}
Google Maps ist der Kartendienst von Google und der Kartenprovider mit den meisten Daten, welche zur Verfügung stehen.
OpenStreetMap ist vergleichsweise auf dem Globus nicht deckend, das liegt, jeder kann aber Daten oder Orte beisteuern.
Google Maps hingegen, hat Daten bis in jede noch so kleine Straße, das ermöglicht eine sehr gute Informationslage.
In Kombination mit Places API und der Maps images API kann niemand mit Google mithalten.
Das liegt auch daran, dass Google mehr als Maps betreut und deshalb viel mehr Möglichkeiten als zum Beispiel OpenStreetMap hat.
Der wohl größte Nachteil liegt an der kostenpflichtigen Nutzung ab einer gewissen Nutzerzahl.
Ab 1000 Anfragen pro Monat ist Google Maps nicht mehr gratis und damit frei zu nutzen~\cite{google-maps-vs-osm}.

\begin{figure}[hbt!]
    \centering
    \includegraphics[scale=0.2]{pics/austria-googlemaps}
    \caption{Österreich mit Bundesländern in Google Maps}
    \label{fig:austria-googlemaps}
\end{figure}

\paragraph{Vorteile}
\begin{itemize}
    \item Google Ökosystem
    \item hohe Abdeckung des Globus
    \item Angular Wrapper
\end{itemize}

\paragraph{Nachteile}
\begin{itemize}
    \item Kann kostenpflichtig werden
    \item Terms of Use und Restriktionen
    \item Google Logo immer sichtbar
    \item Farbschema kann kaum verändert werden.
\end{itemize}

\cleardoublepage

\subsection{Performance im Vergleich}
Um eine Wahl auf ein bestimmtes Framework treffen zu können, wurden mithilfe der Chrome-Developer-Tools die Ladezeiten
unter bestimmten Voraussetzungen der einzelnen Frameworks getestet.
In diesen Tools gibt es die Möglichkeit zwischen einem schnellem (fast), einem mittel-schnellem (mid-tier) und einem
langsamen (low-end) Gerät zu wählen.
Dadurch änderten sich die Ladezeiten teils sehr drastisch.
Folgende Ergebnisse wurden bei dem Versuch erhalten:

\begin{table}[hbt!]
    \centering
    \begin{tabular}{llll}
        & \textbf{Leaflet} & \textbf{Mapbox} & \textbf{Google Maps} \\
        \textbf{low-end device}  & 132s             & 138s            & 192s                 \\
        \textbf{mid-tier device} & 39,5s            & 41,69s          & 56,12s               \\
        \textbf{fast device}     & 1,2s             & 3,9s            & 3,23s
    \end{tabular}\label{tab:map-framework-table}
\end{table}

Aufgrund der oben genannten Vor- und Nachteilen, sowie persönlichen Erfahrungen der Entwickler und nach Absprache mit der
Betreuerfirma ist die Wahl schlussendlich auf Leaflet in Kombination mit einer Mapbox Tile-layer gefallen.
Tile-layers bestimmen die Darstellung der Karte an sich und werden von externen Servern direkt für den aktuell zu sehenden Bereich angefordert.

\cleardoublepage

\section{Static Site Generators}
\setauthor{Sebastian Scholl}
Um die Website zu generieren wurden verschiedene Static Site Generators analysiert.
Dabei wurde darauf geachtet, ob diese sich eignen, aufgrund von Geodaten eine Seite zusammenzubauen.

\subsection{Next.js}
Next.js ist eines der beliebtesten Frameworks, um statische Websites zu generieren.
Die Templates für diese Seiten werden jedoch in React geschrieben.
Vorteile von Next.js sind die einfache Installation und Flexibilität des Frameworks~\cite{nextjs}.

\subsection{Jekyll}
Jekyll ist ein Static Site Generator, der in Ruby geschrieben ist~\cite{jekyll-ruby}.
Im Gegensatz zu Next.js nutzt Jekyll kein JavaScript Framework, um die Templates zu erstellen, sondern setzt
auf \textit{Liquid}~\cite{jekyll-liquid}.
Es besteht zwar die Möglichkeit, es mit Angular zu verknüpfen.
Da aber Liquid und Angular beide mit doppelten geschwungenen Klammern arbeiten, um auf Variablen zuzugreifen, muss
dies in Angular geändert werden.
Das ist zwar grundsätzlich möglich, kann aber eine Sicherheitslücke darstellen und es wird davon abgeraten~\cite{angular-interpolation}.

Weiters wird Jekyll hauptsächlich für Blogs und Seite, die nur Text enthalten, verwendet.
Dementsprechend wird wenig Funktionalität für Seiten mit anderem Inhalt bereitgestellt.

\subsection{Scully}
Scully ist ein Static Site Generator, der auf Angular aufbaut.
Dabei liegt der Fokus jedoch auf serverseitigem Rendern und es werden keine weiteren Tools bereitgestellt, um auf
den Aufbau der Seite Einfluss zu nehmen.
Somit kann zwar die Zeit, die eine Seite braucht, um im Browser dargestellt zu werden, verringert werden, für das
dynamische Generieren der Seite eignet sich Scully aber nicht.

Weiters ist die Community von Scully um einiges kleiner, als die von anderen Static Site Generators.
Bei Problemen und Fragen gibt es daher weniger Ressourcen, auf die man zugreifen kann.

\section{Deployment Pipeline}
\setauthor{Sebastian Scholl}
Nach dem Download des generierten Projekts sollte die Möglichkeit bestehen,
kleine Änderungen daran vorzunehmen und es dann schnell und einfach auf einen
Server zu deployen.
Der Server ist dabei entweder eine Linux Virtual Machine oder eine \textit{Firebase Hosting} Instanz.

Dazu gab es mehrere Ansätze: Ist das Projekt ein Git-Repository, kann eine
Automatisierungssoftware, wie Jenkins oder GitHub Actions verwendet werden,
die bei jedem \textit{Push}-Event das Projekt buildet und auf den Server
deployed.
 Die zweite Möglichkeit ist ein Node.js Skript, das im Projekt enthalten ist und von der Kommandozeile
 aus ausgeführt wird.

\subsection{SSH}
Secure Shell (SSH) ist ein Protokoll, das genutzt wird, um eine sichere Verbindung zwischen einem
Client und einem Server aufzubauen.
 Authentifizierung, Befehle, Output und Dateiübertragung sind dabei verschlüsselt.
 Das Protokoll wird typischerweise benutzt, um Befehle auf einem anderen Computer auszuführen und Dateien sicher
 zu übertragen.
 Dabei eignet es sich gut, um solche Prozesse zu automatisieren und wird daher oft in Continuous Integration und
Continuous Deployment Pipelines verwendet.

Das Protokoll arbeitet mit dem Client-Server-Modell.
 Das heißt, dass die Verbindung von einem SSH Client zu einem SSH Server aufgebaut wird.
 Mit einem Public Key wird die Identität des Servers sichergestellt.
 Nach dem Verbindungsaufbau werden die übertragenen Daten symmetrisch verschlüsselt und mithilfe von
 Hashing-Algorithmen wird die Integrität dieser Daten sichergestellt~\cite{ssh}.

Dateiübertragung über SSH wird mithilfe des SSH File Transfer Protocols (SFTP) realisiert.

\subsubsection{SSH Keys}
Benutzer und Benutzerinnen können zwischen verschiedenen Formen der Authentifizierung wählen.
Die beliebtesten Methoden sind Passwort-basierte Authentifizierung und Public Key Authentifizierung.
Letztere wird vor allem bei automatisierten Prozessen eingesetzt.
Dabei haben der Benutzer oder die Benutzerin ein kryptografisches Schlüsselpaar.
Dieses besteht aus Public Key und Private Key.
Der Public Key wird am Server hinterlegt und jeder mit einer Kopie des dazu passenden Private Keys kann
auf diesen Server zugreifen~\cite{ssh-keys}.

\subsection{Firebase Hosting}
Firebase ist ein Backend-as-a-Service (BaaS), das von Google angeboten wird.
 Es besteht aus verschiedenen Produkten, die Softwareentwicklern und Softwareentwicklerinnen das Entwickeln
von Backends ermöglicht, ohne sich um Server kümmern zu müssen.
 Beispiele für Produkte sind Firebase Authentication, das eine einfache Authentifizierung von Usern ermöglicht,
Realtime Database, eine NoSQL Datenbank oder Firebase Hosting, das das Hosten von Webanwendungen,
statischen Websites oder Microservices übernimmt~\cite{firebase}.

Firebase Hosting ist für eine Verwendung von bis zu 10 GB Speicherplatz gratis~\cite{firebase-pricing}
und stellt mit der Firebase CLI einen einfachen und schnellen Weg zur Verfügung, die gewünschten
Dateien hochzuladen.
Es eignet sich somit für das Hosten der generierten Websites. Weitere Vorteile sind, dass für jedes Firebase
Hosting Projekt eine eigene Domain zur Verfügung gestellt
wird und dass die Website auf das globale Content Delivery Network (CDN) verteilt wird, um
die Ladegeschwindigkeiten zu erhöhen~\cite{firebase-hosting}.

\subsection{Jenkins}
Jenkins ist ein Open-Source Automatisierungsserver.
Das Projekt wird in Java entwickelt und kann mithilfe von Plugins an spezifische Anforderungen
angepasst werden~\cite{jenkins-github}.

Der Server wird vor allem zur Automatisierung von Aufgaben, wie das Builden,
Testen und Deployen von Softwareprojekten genutzt.
Jenkins muss selbst gehosted werden.
 Dazu wird ein offizielles Docker Image im Docker Hub bereitgestellt~\cite{jenkins-dockerhub}.

Die Konfiguration einer Pipeline wird in einem \textit{Jenkinsfile} vorgenommen,
das sich im Git-Repository des Projekts befindet.
 Darin werden mehrere sogenannte Stages definiert, die dann nacheinander ausgeführt werden.
 Jede Stage ist wiederrum in mehrere Steps gegliedert, die verschiedene Aufgaben übernehmen und auch in der
definierten Reihenfolge ausgeführt werden.

Eine Pipeline mit den Stages \textit{Build}, \textit{Test} und \textit{Deploy} würde
zum Beispiel folgendermaßen aussehen: ~\cite{jenkins-jenkinsfile}

\begin{lstlisting}[numbers=left]
pipeline {
    agent any

    stages {
        stage('Build') {
            steps {
                echo 'Building..'
            }
        }
        stage('Test') {
            steps {
                echo 'Testing..'
            }
        }
        stage('Deploy') {
            steps {
                echo 'Deploying....'
            }
        }
    }
}
\end{lstlisting}

\subsection{GitHub Actions}
GitHub Actions ist eine Automatisierungssoftware, die von GitHub zur Verfügung
gestellt wird.
Im Gegensatz zu Jenkins werden die Server dabei von GitHub gehostet.
Die Konfiguration von sogenannten Workflows wird in \textit{YAML}-Dateien im
\textit{.github/workflows} Directory eines Repositories vorgenommen.

GitHub Actions sind tief in das GitHub Ökosystem integriert und bieten viele Möglichkeiten,
auf verschiedene Teile davon zuzugreifen.
 Damit können Workflows erstellt werden, die nicht nur die Software builden, testen und deployen, sondern auch
 zum Beispiel die Labels von Issues verändern.

Ein Workflow enthält einen oder mehrere Jobs, die sequentiell oder parallel ausgeführt werden
können.
 Jeder Job läuft auf einer eigenen Virtuellen Maschine, auch Runner genannt und ist in
verschiedene Actions unterteilt, die wiederrum mehrere Steps enthalten.
 Dabei läuft
auf den Runnern entweder Windows, Linux oder macOS als Betriebssystem.

Ein Workflow, der bei jedem Push oder Pull Request auf den \textit{main} Branch die Steps
\textit{Build}, \textit{Test} und \textit{Deploy} ausführt könnte beispielsweise folgendermaßen
aussehen: ~\cite{github-actions}

\begin{lstlisting}[numbers=left]
name: Build & Deploy

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]

jobs:
  build-deploy:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v2
    - name: Build
      run: echo Building
    - name: Test
      run: echo Testing
    - name: Deploy
      run: echo Deploy
\end{lstlisting}

\subsubsection{Secrets}
Secrets sind verschlüsselte Umgebungsvariablen, die für eine GitHub Organisation oder ein
Repository erstellt werden.
Darin können sensible Informationen, wie z.B. Passwörter oder Private Keys gespeichert werden.
Diese Secrets sind dann in GitHub Actions Workflows verfügbar.
GitHub stellt sicher, dass Secrets verschlüsselt werden, bevor sie die GitHub Server erreichen
und verschlüsselt bleiben, bis sie in einem Workflow verwendet werden~\cite{github-secrets}.

\begin{figure}[hbt!]
    \centering
    \includegraphics[scale=0.5]{pics/secrets.png}
    \caption{Repository secrets}
\end{figure}

In folgendem Step eines Workflows werden diese Secrets dann referenziert:

\begin{lstlisting}[numbers=left]
- name: Deploy to Server
  uses: easingthemes/ssh-deploy@v2
  env:
    SSH_PRIVATE_KEY: ${{ secrets.SSH_PRIVATE_KEY }}
    SOURCE: static/
    REMOTE_HOST: ${{ secrets.REMOTE_HOST }}
    REMOTE_USER: ${{ secrets.REMOTE_USER }}
    TARGET: ${{ secrets.REMOTE_TARGET }}
\end{lstlisting}

\subsection{Node.js Skripts}
Node.js~\ref{Node.js} Skripts haben den Vorteil gegenüber dem Ansatz mit einer Automatisierungssoftware,
dass dafür kein GitHub Repository erstellt werden muss und nur die Kommandozeile benötigt wird.
Da Node.js plattformunabhängig ist, können diese Skripts in sowohl auf Windows als auch auf
Linux und macOS ausgeführt werden.

 \section{JSON}
 JavaScript Object Notation (JSON) ist ein Format zum Datenaustausch.
 Es wurde entworfen, um für Menschen einfach zu lesen und zu schreiben zu sein.
 Außerdem ist es auch für Maschinen einfach zu parsen oder zu generieren.
 Das Format ist unabhängig von Programmiersprachen.

 JSON baut auf zwei Strukturen auf:

 \begin{itemize}
     \item Object: Eine Sammlung an Key/Value Paaren.
     In den meisten Programmiersprachen wird das als Object, Record, Struct
     oder Dictionary umgesetzt.
     \item Array: Eine geordnete Sammlung an Werten.
     Diese Struktur wird als Array, Vector, List oder Sequence umgesetzt.
 \end{itemize}

 Ein Object ist ein ungeordnetes Set an Key/Value Paaren.
 Es beginnt mit \textit{{} und endet mit \textit{}}.
 Die darin enthaltenen Werte haben das Format \textit{key: value} und werden mit Beistrichen voneinander getrennt.
 Der Key ist immer ein String und muss deshalb in Anführungszeichen stehen.

 Ein Array ist eine geordnete Sammlung an Werten.
 Es startet mit \textit{[} und wird mit \textit{]} beendet.
 Die darin enthaltenen Werte werden wiederrum mit Beistrichen getrennt.

 Werte können ein String sein, der in Anführungszeichen steht, eine Zahl, \textit{true}, \textit{false}, \textit{null}
 oder wieder ein Object oder Array~\cite{json}.

 Ein valides JSON-Object könnte also folgendermaßen aussehen:

 \label{json-example}
 \begin{lstlisting}[numbers=left]
{
  "string": "Hello World",
  "number": 69,
  "boolean": false,
  "null": null,
  "array": [
    419,
    420,
    "421",
    null
  ]
}
 \end{lstlisting}

 \subsection{JSON-Schema}
Ein JSON Schema wird dazu verwendet, JSON Strukturen zu beschreiben und zu validieren.
Damit kann beispielsweise eine Dokumentation für eine Schnittstelle, die über JSON kommuniziert, definiert werden.
Diese Dokumentation ist dabei sowohl für Menschen, als auch für Maschinen einfach lesbar.
Ein weiterer Einsatzbereich von JSON-Schemata ist das Validieren von Daten bei automatisierten Tests~\cite{json-schema}.

 Ein JSON-Schema ist selbst wiederrum nur ein JSON-Object.
 Dieses besitzt meistens einen \textit{title} und eine \textit{description}, die das definierte JSON beschreiben,
 aber keine Auswirkung auf die Struktur haben.
 Das \textit{type} Keyword definiert den Typ einer Property.
 Dieser kann entweder "null", "boolean", "object", "array", "number", oder "string" sein.

 Ist der Typ "object", muss dieses Object weiter beschrieben werden.
 Dazu wird das \textit{properties} Keyword verwendet.
 Der Wert davon ist ein Object mit den Properties des zu beschreibenden Object als Keys.
 Die Werte dieser Keys sind wiederrum Objects, die die jeweilige Property beschreiben.
 Dazu wird wieder das \textit{type} Keyword verwendet.
 Mit dem \textit{description} Keyword kann man auch dieser Property wieder eine Beschreibung geben.

 Eine Property vom Typ "array" beschreibt mit dem \textit{items} Keyword den Typ der Werte, die das Array enthält.

 Für die meisten Typen gibt es noch zusätzliche Keywords, mit denen die Werte noch genauer beschrieben werden können.
 Dazu zählen beispielsweise \textit{required} für Werte vom Typ "object", das ein Array der Namen der erforderlichen
 Properties enthält oder \textit{maximum} und \textit{minimum} für Werte vom Typ "number" bzw. "integer".

 Ein JSON-Schema für das JSON-Object aus dem vorherigen Kapitel~\ref{json-example} könnte beispielsweise
 folgendermaßen aussehen:

 \begin{lstlisting}[numbers=left]
    {
      "title": "Example JSON Object",
      "description": "A JSON Object to demonstrate JSON Syntax",
      "type": "object",
      "properties": {
        "string": {
          "description": "Some string",
          "type": "string"
        },
        "number": {
          "description": "Some number",
          "type": "integer",
          "maximum": 100
        },
        "boolean": {
          "description": "Some boolean",
          "type": "boolean"
        },
        "null": {
          "description": "Null",
          "type": "null"
        },
        "array": {
          "description": "Some array",
          "type": "array",
          "items": {
            "oneOf": [
              {
                "type": "number"
              },
              {
                "type": "string"
              },
              {
                "type": "null"
              }
            ]
          }
        }
      },
      "required": [ "string", "number", "array" ]
    }
 \end{lstlisting}

\section{Backend}
\setauthor{Sebastian Scholl}
Das Backend von webmap erfüllt zwei Aufgaben:

\begin{itemize}
    \item Das Hosten des Generators
    \item Eine REST-API für das Generieren eines Angular-Projekts aufgrund der im Generator erstellten Konfiguration
\end{itemize}

Für das Hosting wurde nginx verwendet und die REST-API wurde in TypeScript implementiert.

\subsection{TypeScript}
Um webmap leicht warten zu können, wurde auch das Backend in TypeScript~\ref{TypeScript} geschrieben.
 Damit wird nur eine Programmiersprache für das gesamte Projekt verwendet.
 Ausgeführt wird das Backend in der Laufzeitumgebung Node.js.
 Diese wurde entwickelt, um skalierbare Netzwerkanwendungen zu erstellen und eignet sich daher für
 die REST-API~\cite{about-node-js}.

\subsubsection{express.js}
Die API wurde mit dem Node-Package \textit{express.js} entwickelt.
 Express ist ein schnelles und minimalistisches Web Framework.
 Die Kernfunktion liegt darin, schnelle und robuste APIs zu entwickeln~\cite{expressjs}.

\subsubsection{node-tar}
Zum Komprimieren des generierten Projekts wird das Node-Package \textit{node-tar} verwendet.
 Das Package ahmt dabei den UNIX-Befehl \textit{tar} nach~\cite{node-tar}.

Ein \textit{tar}-File oder \textit{tarball} ist ein Archiv von Filesystem-Einträgen wie Dateien, Directories oder
Symbolic Links.
 Diese können dann noch komprimiert werden, um ein \textit{.tar.gz}-File zu erhalten.
 Tar wird hauptsächlich zur Distribution oder für Backups von mehreren Dateien verwendet~\cite{tar-manpage}.

 \subsubsection{ESLint}
 ESLint ist ein Open-Source Linting Tool für JavaScript und TypeScript.
 Es wird verwendet, um geschriebenen Code zu analysieren, um Muster zu finden, von denen man weiß, dass sie
 problematisch und anfällig für Bugs sind.
 Weiters kann man damit einen bestimmten Code Style definieren, um den Code konsistent und lesbar zu machen.
 Alle diese Aspekte werden als sogenannte Rules definiert.
 Die Library stellt eine Menge an vorgefertigten Rules zur Verfügung, es können aber auch sehr einfach eigene
 Rules erstellt werden~\cite{eslint}.

\section{nginx}
\setauthor{Sebastian Scholl}
nginx ist ein Open-Source HTTP Server, Reverse Proxy Server, Mail Proxy Server und generischer TCP/UDP Proxy Server.
Konfigurationen werden im \textit{/etc/nginx/nginx.conf} File durchgeführt.
 Standardmäßig stellt der Webserver alle Dateien im \textit{/usr/share/nginx/html} Directory Clients zur
 Verfügung~\cite{nginx}.
 Auch für nginx steht ein offizielles Docker Image am Docker Hub zur Verfügung~\cite{nginx-dockerhub}.

\subsubsection{Reverse Proxy}
Ein Reverse Proxy Server wie nginx befindet sich typischerweise hinter einer Firewall in einem
privaten Netzwerk und verteilt Requests von Clients auf die entsprechenden Backend Server.
Damit erhält man eine weitere Abstraktionsschicht, um den Verkehr zwischen Clients und Servern zu
verwalten und zu überwachen~\cite{nginx-proxy-server}.

nginx kann mit folgendem \textit{nginx.conf} File als Proxy Server konfiguriert werden, der Requests zu \textit{/api}
an einen anderen Server weiterleitet.
Bei allen anderen Requests wird eine Datei aus dem \textit{/data} Directory als Response gesendet.

\begin{lstlisting}[numbers=left]
events {}

http {
  include mime.types;

  server {
    location /api {
      proxy_pass http://localhost:4000/;
    }

    location / {
      root /data;
    }
  }
}
\end{lstlisting}

\section{Containerization}
\setauthor{Sebastian Scholl}
\subsection{Docker}
 Docker ist eine Softwareplattform, die Benutzern und Benutzerinnen erlaubt, Applikationen schnell zu
 Builden, Testen und Deployen.
 Dabei wird die Software in standardisierte Pakete, die Container genannt werden, verpackt~\cite{docker-aws}.

\subsubsection{Container}
 Container sind abgeschlossene Systeme, die Software sowie die gesamte Infrastruktur, die diese Software
 benötigt, um zu laufen, enthält.
 Dazu gehören Libraries, sowie eine Laufzeitumgebung.
 Die Idee von Containern erinnert an Virtuelle Maschinen.
 Bei Containern wird jedoch das Betriebssystem virtualisiert, im Gegensatz zu Virtuellen Maschinen,
 bei denen die Hardware virtualisiert wird.
 Deshalb benötigt jede VM eine eigene Kopie eines Betriebssystemes, was mehrere GB an Speicherplatz beansprucht.
 Container haben hingegen typischerweise eine Größe von weniger als 100 MB~\cite{docker-container}.

 \begin{figure}[hbt!]
     \centering
     \includegraphics[scale=0.2]{pics/docker-vm.png}
     \caption{Virtualisierung mit Virtuellen Maschinen}
     % Source: https://www.docker.com/resources/what-container/, 25.03.
 \end{figure}

 \begin{figure}[hbt!]
     \centering
     \includegraphics[scale=0.2]{pics/docker-containerized.png}
     \caption{Virtualisierung mit Containern}
     % Source: https://www.docker.com/resources/what-container/, 25.03.
 \end{figure}

\subsubsection{Image}
 Container werden aus sogenannten Images erstellt.
 Diese Images erhalten alles, was ein Container benötigt, um zu laufen.
 Dazu gehören das Filesystem mit allen Dependencies und Konfigurationen, Umgebungsvariablen
 und einen Befehl, um die Software im Container zu starten.
 Beim Start des Containers wird der Inhalt des Images kopiert und darin der Befehl ausgeführt, der die Software
 startet.
 Dafür wird der Docker CLI Befehl \textit{docker run [image name]} verwendet~\cite{docker-image}.

 Images können entweder von einem Registry heruntergeladen oder selbst in einem sogenannten \textit{Dockerfile}
 definiert werden.
 Jedes \textit{Dockerfile} startet mit einem Base Image, auf dem aufgebaut wird.
 Danach können zum Beispiel Files in das Image kopiert werden, aus denen dann eine Applikation gebuildet wird.
 Zum Schluss benötigt ein \textit{Dockerfile} noch einen Befehl, der beim Start eines Containers ausgeführt wird.

 Folgendes Dockerfile verwendet das Base Image \textit{ubuntu}.
 Das gesamte Directory, in dem sich das \textit{Dockerfile} befindet, wird daraufhin in das \textit{/app} Directory
 des Images kopiert.
 Darin wird der \textit{make} Befehl ausgeführt, um die Applikation zu builden.
 Als Befehl, der beim Start ausgeführt wird, wird \textit{python /app/app.py} definiert~\cite{dockerfile}.

 \begin{lstlisting}[numbers=left]
  FROM ubuntu:18.04
  COPY . /app
  RUN make /app
  CMD python /app/app.py
 \end{lstlisting}

 \subsubsection{Persistierung}
 Docker bietet zwei Wege an, um Daten, die in Containern generiert werden, zu persistieren.
 Bind Mounts synchronisieren ein Directory im lokalen Filesystem mit einem Directory im Filesystem des Containers.
 Diese Bind Mounts werden jedoch öfter dazu verwendet, um Daten von außerhalb in den Container zu kopieren.

 Um Daten, die im Container generiert werden, zu persistieren werden meistens Volumes verwendet.
 Volumes enthalten ein Directory des Containers und werden vollständig von Docker verwaltet.
 Wird der Container gestoppt oder gelöscht, bleiben die Daten im Volume erhalten.

 Volumes werden beim Erstellen des Containers konfiguriert.
 Die Docker CLI stellt dafür die \textit{-v} bzw. \textit{--volume} Flag zur Verfügung~\cite{docker-volumes}.

 \subsubsection{Microservices}
 Microservices beschreibt einen Ansatz in der Softwareentwicklung, bei dem die Software in kleine, unabhängige
 Services aufgeteilt ist, die über definierte APIs miteinander kommunizieren.
 Diese Architektur machen Applikationen leichter skalierbar.
 Diese Services werden oft als Docker Container realisiert.

 Das Gegenteil der Microservice Architektur ist ein Monolith.
 Dabei ist die gesamte Applikation ein einzelner Service.
 Dieser enthält alle Komponenten, die eng miteinander verbunden sind.
 Bei erhöhter Auslastung muss dann die gesamte Applikation skaliert werden.
 Jede zusätzliche Komponente erhöht außerdem die Komplexität des gesamten Services.
 Das macht es schwieriger, mit neuen Technologien zu experimentieren und neue Ideen einzubauen.
 Durch die Abhängigkeit der Komponenten voneinander kann ein einzelner Fehler in einem Teil des Services eine
 große Auswirkung auf die gesamte Applikation haben.

 Microservices sollen diese Probleme lösen.
 Die Komponenten funktionieren unabhängig von einander und so muss bei der erhöhten Auslastung eines Services nur
 dieser skaliert werden.
 Da die Services über eine definierte API miteinander kommunizieren kann die Technologie in den Services einfach
 ausgetauscht werden.
 Weiters beschränken sich die Ausmaße von Fehlern nur auf den Service, in dem sie auftreten.

 Mit der steigenden Verbreitung von Cloud Computing werden Microservices immer beliebter, da die Skalierung
 von Services dabei automatisiert werden kann~\cite{microservices}.

 \begin{figure}[hbt!]
     \centering
     \includegraphics[scale=0.5]{pics/microservice.png}
     \caption{Monolith vs. Microservices}
     % Source: https://aws.amazon.com/microservices/, 25.03.
 \end{figure}

 \subsubsection{Docker Hub}
 Docker Images können in sogenannte Registries hochgeladen und so verteilt werden~\cite{docker-registry}.
 Das größte Registry ist der Docker Hub~\cite{docker-hub}.
 Darin werden auch offizielle Images für Softwarepakete, wie Ubuntu, nginx oder Node.js zur Verfügung gestellt.
 Diese Images werden immer auf dem neusten Stand gehalten, um die Sicherheit zu
 gewährleisten~\cite{docker-official-images}.

\subsection{Docker Compose}

 Docker Compose ist ein Tool zur Orchestrierung von Docker Containern.
 Es wird verwendet, um Applikationen mit mehreren Containern zu verwalten.
 In einem \textit{YAML} File werden die Services der Applikation definiert, die dann alle mit einem einzelnen
 Befehl gestartet werden können~\cite{docker-compose}.

 \subsubsection{Networking}
 Alle Services, die im gleichen File definiert werden, befinden sich automatisch im selben Docker Netzwerk.
 Das bedeutet, dass die miteinander kommunizieren können, aber nicht von außen auf sie zugegriffen werden kann.
 Jeder Service kann jedoch einzelne Ports definieren, die auch von außerhalb erreichbar sein sollen.
 Diese Abstraktionsschicht erhöht die Sicherheit, da zum Beispiel nur Services innerhalb eines Docker Netzwerks auf eine
 Datenbank zugreifen können, diese von Prozessen außerhalb nicht erreichbar ist.
 Docker Compose erleichtert das Routing innerhalb der Docker Netzwerke außerdem, indem der Hostname eines Services
 gleich seinem Namen ist~\cite{docker-network}.
